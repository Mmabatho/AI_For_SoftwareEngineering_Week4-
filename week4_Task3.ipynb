{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":72815,"databundleVersionId":8003195,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Kaggle Notebook: Predictive Analytics for Resource Allocation\n# Breast Cancer Dataset Analysis\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"PREDICTIVE ANALYTICS FOR RESOURCE ALLOCATION ===\")\nprint(\"Dataset: Breast Cancer Dataset\")\nprint(\"Goal: Predict resource allocation priority based on diagnostic features\\n\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:41:31.764665Z","iopub.execute_input":"2025-06-19T21:41:31.764938Z","iopub.status.idle":"2025-06-19T21:41:31.770955Z","shell.execute_reply.started":"2025-06-19T21:41:31.764923Z","shell.execute_reply":"2025-06-19T21:41:31.770265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# 1. DATA LOADING AND EXPLORATION\n# ============================================================================","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:13:41.324923Z","iopub.execute_input":"2025-06-19T21:13:41.325216Z","iopub.status.idle":"2025-06-19T21:13:41.328522Z","shell.execute_reply.started":"2025-06-19T21:13:41.325197Z","shell.execute_reply":"2025-06-19T21:13:41.327877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"1. LOADING AND EXPLORING DATA\")\nprint(\"-\" * 50)\n\n# Load the breast cancer dataset\ndata = load_breast_cancer()\ndf = pd.DataFrame(data.data, columns=data.feature_names)\ndf['diagnosis'] = data.target\n\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Features: {len(data.feature_names)}\")\nprint(f\"Target classes: {data.target_names}\")\nprint(\"\\nFirst few rows:\")\nprint(df.head())\n\nprint(\"\\nDataset Info:\")\nprint(df.info())\n\nprint(\"\\nMissing values:\")\nprint(df.isnull().sum().sum())\n\nprint(\"\\nTarget distribution:\")\nprint(df['diagnosis'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:15:04.729259Z","iopub.execute_input":"2025-06-19T21:15:04.729563Z","iopub.status.idle":"2025-06-19T21:15:04.782994Z","shell.execute_reply.started":"2025-06-19T21:15:04.729540Z","shell.execute_reply":"2025-06-19T21:15:04.782296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# 2. DATA PREPROCESSING\n# ============================================================================\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:15:54.605290Z","iopub.execute_input":"2025-06-19T21:15:54.605845Z","iopub.status.idle":"2025-06-19T21:15:54.609086Z","shell.execute_reply.started":"2025-06-19T21:15:54.605821Z","shell.execute_reply":"2025-06-19T21:15:54.608331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n2. DATA PREPROCESSING\")\nprint(\"-\" * 50)\n\n# Create priority labels based on diagnosis and feature severity\n# We'll use mean radius, mean texture, and mean perimeter to determine priority\ndef assign_priority(row):\n    \"\"\"\n    Assign priority based on diagnosis and severity indicators\n    High: Malignant cases with high severity indicators\n    Medium: Malignant cases with moderate severity or Benign with high indicators\n    Low: Benign cases with low severity indicators\n    \"\"\"\n    diagnosis = row['diagnosis']  # 0 = malignant, 1 = benign\n    mean_radius = row['mean radius']\n    mean_texture = row['mean texture']\n    mean_perimeter = row['mean perimeter']\n    \n    # Calculate severity score (normalized)\n    severity_score = (mean_radius + mean_texture + mean_perimeter) / 3\n    \n    if diagnosis == 0:  # Malignant\n        if severity_score > df[['mean radius', 'mean texture', 'mean perimeter']].mean().mean() * 1.2:\n            return 'High'\n        else:\n            return 'Medium'\n    else:  # Benign\n        if severity_score > df[['mean radius', 'mean texture', 'mean perimeter']].mean().mean() * 1.1:\n            return 'Medium'\n        else:\n            return 'Low'\n\n# Apply priority assignment\ndf['priority'] = df.apply(assign_priority, axis=1)\n\nprint(\"Priority distribution:\")\nprint(df['priority'].value_counts())\n\n# Encode priority labels\nlabel_encoder = LabelEncoder()\ndf['priority_encoded'] = label_encoder.fit_transform(df['priority'])\npriority_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\nprint(f\"\\nPriority encoding: {priority_mapping}\")\n\n# Select features for modeling (top 10 most important features)\nfeature_cols = [\n    'mean radius', 'mean texture', 'mean perimeter', 'mean area',\n    'mean smoothness', 'mean compactness', 'mean concavity',\n    'mean concave points', 'mean symmetry', 'mean fractal dimension'\n]\n\nX = df[feature_cols]\ny = df['priority_encoded']\n\nprint(f\"\\nFeatures selected: {len(feature_cols)}\")\nprint(\"Feature statistics:\")\nprint(X.describe())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:18:53.886228Z","iopub.execute_input":"2025-06-19T21:18:53.886747Z","iopub.status.idle":"2025-06-19T21:18:54.246551Z","shell.execute_reply.started":"2025-06-19T21:18:53.886724Z","shell.execute_reply":"2025-06-19T21:18:54.245912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# 3. DATA SPLITTING AND SCALING\n# ============================================================================","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:19:48.484864Z","iopub.execute_input":"2025-06-19T21:19:48.485480Z","iopub.status.idle":"2025-06-19T21:19:48.489239Z","shell.execute_reply.started":"2025-06-19T21:19:48.485434Z","shell.execute_reply":"2025-06-19T21:19:48.488629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n3. DATA SPLITTING AND SCALING\")\nprint(\"-\" * 50)\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"Training set size: {X_train.shape[0]}\")\nprint(f\"Test set size: {X_test.shape[0]}\")\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(\"Data scaling completed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:20:31.048649Z","iopub.execute_input":"2025-06-19T21:20:31.049275Z","iopub.status.idle":"2025-06-19T21:20:31.063231Z","shell.execute_reply.started":"2025-06-19T21:20:31.049248Z","shell.execute_reply":"2025-06-19T21:20:31.062496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# 4. MODEL TRAINING\n# ============================================================================","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n4. MODEL TRAINING\")\nprint(\"-\" * 50)\n\n# Train Random Forest model\nrf_model = RandomForestClassifier(\n    n_estimators=100,\n    random_state=42,\n    max_depth=10,\n    min_samples_split=5,\n    min_samples_leaf=2\n)\n\nprint(\"Training Random Forest model...\")\nrf_model.fit(X_train_scaled, y_train)\nprint(\"Model training completed\")\n\n# Feature importance\nfeature_importance = pd.DataFrame({\n    'feature': feature_cols,\n    'importance': rf_model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nTop 5 most important features:\")\nprint(feature_importance.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:22:37.484798Z","iopub.execute_input":"2025-06-19T21:22:37.485490Z","iopub.status.idle":"2025-06-19T21:22:37.656188Z","shell.execute_reply.started":"2025-06-19T21:22:37.485467Z","shell.execute_reply":"2025-06-19T21:22:37.655638Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# 5. MODEL EVALUATION\n# ============================================================================","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n5. MODEL EVALUATION\")\nprint(\"-\" * 50)\n\n# Make predictions\ny_pred = rf_model.predict(X_test_scaled)\n\n# Calculate metrics\naccuracy = accuracy_score(y_test, y_pred)\nf1_macro = f1_score(y_test, y_pred, average='macro')\nf1_weighted = f1_score(y_test, y_pred, average='weighted')\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1-Score (Macro): {f1_macro:.4f}\")\nprint(f\"F1-Score (Weighted): {f1_weighted:.4f}\")\n\n# Detailed classification report\nprint(\"\\nDetailed Classification Report:\")\ntarget_names = ['High', 'Low', 'Medium']  # Based on label encoding\nprint(classification_report(y_test, y_pred, target_names=target_names))\n\n# Confusion Matrix\nprint(\"\\nConfusion Matrix:\")\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\n# Per-class metrics\nprecision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred, average=None)\nmetrics_df = pd.DataFrame({\n    'Priority': target_names,\n    'Precision': precision,\n    'Recall': recall,\n    'F1-Score': f1,\n    'Support': support\n})\nprint(\"\\nPer-class Performance Metrics:\")\nprint(metrics_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:24:26.007223Z","iopub.execute_input":"2025-06-19T21:24:26.007579Z","iopub.status.idle":"2025-06-19T21:24:26.036202Z","shell.execute_reply.started":"2025-06-19T21:24:26.007559Z","shell.execute_reply":"2025-06-19T21:24:26.035623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# 6. RESULTS SUMMARY\n# ============================================================================","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n6. RESULTS SUMMARY\")\nprint(\"=\" * 50)\n\nprint(\"PERFORMANCE METRICS:\")\nprint(f\"• Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\nprint(f\"• Macro F1-Score: {f1_macro:.4f}\")\nprint(f\"• Weighted F1-Score: {f1_weighted:.4f}\")\n\nprint(\"\\nMODEL INSIGHTS:\")\nprint(f\"• Total samples processed: {len(df)}\")\nprint(f\"• Training samples: {len(X_train)}\")\nprint(f\"• Test samples: {len(X_test)}\")\nprint(f\"• Number of features used: {len(feature_cols)}\")\n\nprint(\"\\nPRIORITY DISTRIBUTION IN TEST SET:\")\ntest_priority_dist = pd.Series(y_test).value_counts().sort_index()\nfor idx, count in test_priority_dist.items():\n    priority_name = target_names[idx]\n    print(f\"• {priority_name}: {count} samples ({count/len(y_test)*100:.1f}%)\")\n\nprint(\"\\nTOP 3 MOST IMPORTANT FEATURES:\")\nfor i, row in feature_importance.head(3).iterrows():\n    print(f\"• {row['feature']}: {row['importance']:.4f}\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"DELIVERABLE COMPLETED: Jupyter Notebook with Performance Metrics\")\nprint(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:26:00.088136Z","iopub.execute_input":"2025-06-19T21:26:00.088713Z","iopub.status.idle":"2025-06-19T21:26:00.097386Z","shell.execute_reply.started":"2025-06-19T21:26:00.088689Z","shell.execute_reply":"2025-06-19T21:26:00.096750Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set style\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\nprint(\"GENERATING VISUALIZATIONS\")\nprint(\"-\" * 50)\n\n# Load and prepare data (same as main script)\ndata = load_breast_cancer()\ndf = pd.DataFrame(data.data, columns=data.feature_names)\ndf['diagnosis'] = data.target\n\n# Create priority labels (same logic as main script)\ndef assign_priority(row):\n    diagnosis = row['diagnosis']\n    mean_radius = row['mean radius']\n    mean_texture = row['mean texture']\n    mean_perimeter = row['mean perimeter']\n    severity_score = (mean_radius + mean_texture + mean_perimeter) / 3\n    \n    if diagnosis == 0:  # Malignant\n        if severity_score > df[['mean radius', 'mean texture', 'mean perimeter']].mean().mean() * 1.2:\n            return 'High'\n        else:\n            return 'Medium'\n    else:  # Benign\n        if severity_score > df[['mean radius', 'mean texture', 'mean perimeter']].mean().mean() * 1.1:\n            return 'Medium'\n        else:\n            return 'Low'\n\ndf['priority'] = df.apply(assign_priority, axis=1)\n\n\n# Create visualizations\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Breast Cancer Dataset - Resource Allocation Analysis', fontsize=16, fontweight='bold')\n\n# 1. Priority Distribution\naxes[0, 0].pie(df['priority'].value_counts().values, \n               labels=df['priority'].value_counts().index,\n               autopct='%1.1f%%', startangle=90)\naxes[0, 0].set_title('Priority Distribution')\n\n# 2. Diagnosis vs Priority\npriority_diagnosis = pd.crosstab(df['priority'], df['diagnosis'])\npriority_diagnosis.plot(kind='bar', ax=axes[0, 1], color=['lightcoral', 'lightblue'])\naxes[0, 1].set_title('Priority vs Diagnosis')\naxes[0, 1].set_xlabel('Priority Level')\naxes[0, 1].set_ylabel('Count')\naxes[0, 1].legend(['Malignant', 'Benign'])\naxes[0, 1].tick_params(axis='x', rotation=45)\n\n# 3. Feature correlation heatmap (top features)\ntop_features = ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean compactness']\ncorr_matrix = df[top_features].corr()\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1, 0])\naxes[1, 0].set_title('Feature Correlation Matrix')\n\n# 4. Box plot of key features by priority\ndf_melted = df[['mean radius', 'mean texture', 'mean perimeter', 'priority']].melt(\n    id_vars=['priority'], var_name='feature', value_name='value'\n)\nsns.boxplot(data=df_melted, x='priority', y='value', hue='feature', ax=axes[1, 1])\naxes[1, 1].set_title('Key Features by Priority Level')\naxes[1, 1].tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Visualizations generated successfully!\")\nprint(\"\\nKey Insights:\")\nprint(\"1. Priority distribution shows balanced allocation across categories\")\nprint(\"2. High priority cases are predominantly malignant diagnoses\")\nprint(\"3. Feature correlations help identify redundant measurements\")\nprint(\"4. Box plots reveal clear separation between priority levels\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:33:46.524832Z","iopub.execute_input":"2025-06-19T21:33:46.525091Z","iopub.status.idle":"2025-06-19T21:33:47.785345Z","shell.execute_reply.started":"2025-06-19T21:33:46.525075Z","shell.execute_reply":"2025-06-19T21:33:47.784484Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"MODEL COMPARISON FOR RESOURCE ALLOCATION PREDICTION\")\nprint(\"-\" * 60)\n\n# Assuming X_train_scaled, X_test_scaled, y_train, y_test are available from main script\n# This would be run after the main preprocessing\n\nmodels = {\n    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n    'SVM': SVC(kernel='rbf', random_state=42),\n    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n}\n\nresults = []\n\nfor name, model in models.items():\n    print(f\"\\nTraining {name}...\")\n    \n    # Train model\n    model.fit(X_train_scaled, y_train)\n    \n    # Predictions\n    y_pred = model.predict(X_test_scaled)\n    \n    # Metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    f1_macro = f1_score(y_test, y_pred, average='macro')\n    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n    \n    # Cross-validation\n    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n    \n    results.append({\n        'Model': name,\n        'Accuracy': accuracy,\n        'F1_Macro': f1_macro,\n        'F1_Weighted': f1_weighted,\n        'CV_Mean': cv_scores.mean(),\n        'CV_Std': cv_scores.std()\n    })\n    \n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"F1-Score (Macro): {f1_macro:.4f}\")\n    print(f\"CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n\n# Results summary\nresults_df = pd.DataFrame(results)\nresults_df = results_df.sort_values('F1_Macro', ascending=False)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"MODEL COMPARISON RESULTS\")\nprint(\"=\"*60)\nprint(results_df.to_string(index=False, float_format='%.4f'))\n\nbest_model = results_df.iloc[0]['Model']\nprint(f\"\\nBest performing model: {best_model}\")\nprint(f\"Best F1-Score (Macro): {results_df.iloc[0]['F1_Macro']:.4f}\")\n\nprint(\"\\nRECOMMENDATION:\")\nprint(f\"Use {best_model} for production deployment\")\nprint(\"Consider ensemble methods for improved robustness\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:42:00.884585Z","iopub.execute_input":"2025-06-19T21:42:00.884859Z","iopub.status.idle":"2025-06-19T21:42:05.286892Z","shell.execute_reply.started":"2025-06-19T21:42:00.884841Z","shell.execute_reply":"2025-06-19T21:42:05.284369Z"}},"outputs":[],"execution_count":null}]}